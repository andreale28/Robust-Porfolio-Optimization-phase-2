\documentclass[a4paper,12pt,twoside]{article}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=3cm]{geometry}
\usepackage{dirtytalk}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath} % assumes amsmath package installed


\setcounter{section}{0}
\renewcommand{\baselinestretch}{1.4}
\begin{document}
\section{Introduction}
Investment in an individual security always has an associated risk, which can be minimized through diversification, a process involving  investment in a portfolio consisting of several securities. In accordance with this notion, Markowitz \cite{Markowitz1,Markowitz2} introduced the mean-variance model for optimum allocation of weights to the securities comprising a portfolio. Mean and covariance matrix of returns of securities are used as the measures for giving a quantitative sense to the return and the risk of the portfolio respectively. Despite the theoretical simplicity of the approach of Markowitz model based on risk-return trade-off, there are several drawbacks associated with incorporating it in a practical setup.\\

Theoretically, Markowitz based portfolio optimization can result in assigning extreme weights to the securities comprising a portfolio. However, investment in securities can't be made in such extreme positions like large short positions if one takes the active trading into account. Such kind of scenarios can be avoided by introducing appropriate constraints on the weights.  Black and Litterman \cite{Black} argue that there is an added disadvantage since there are high chances of the optimal portfolio lying in the neighborhood of the imposed constraints. Thus, imposition of constraints leads to strong dependence of the constructed portfolio upon the constraints. For example, disallowing short sales often results in assigning zero weights to many securities and largely positive weights to the securities having small market capitalizations. \\

One of the most significant demerits of the mean-variance model is the sensitivity issue associated with risk and return parameters of the individual securities of a portfolio. These parameters are estimated using mean and standard deviation of returns. While computing these estimates, historical data of returns is usually taken into account in order to calculate the sample mean and the sample variance. The historical data neglects various other market factors and is not an accurate representation for estimates of future returns. Taking into account the above reasons, Michaud \cite{Michaud} argues that the mean-variance analysis tends to maximize the impact of estimation errors associated with the return and the risk parameters for the securities. As a result, Markowitz portfolio optimization often overweighs (underweighs) the securities having higher (lower) expected return, lower (higher) variance of returns and negative (positive) correlation between their returns. Labelling the model as \say{estimation-error maximizers}, he states that it often leads to financially counter-intuitive portfolios, which, in some cases, perform worse than the equal-weighted portfolio. Broadie \cite{Broadie} investigates the error maximization property of mean-variance analysis. Accordingly, he conducts a simulation based study to compare the estimated efficient frontier with the actual frontier computed using true parameter values. He observes that points on the estimated efficient frontier show superior performance as compared to the corresponding points on the actual frontier. He supports his argument of over-estimation of expected returns of optimal portfolios through his simulated results of obtaining the estimated frontier lying above the actual frontier. Additionally, he points out that non-stationarity in the data of returns can further increase the errors in computing the efficient frontier. Chopra and Ziemba \cite{Chopra} perform the sensitivity analysis of performance of optimal portfolios by studying the relative effect of estimation errors in means, variances and covariances of security returns, taking the investors' risk tolerance into consideration as well. They observe that at a high risk tolerance (to be defined in later chapters) of around fifty, cash equivalent loss for estimation errors in means is about eleven times greater than that for errors in variances or covariances. Accordingly, they point out that if the investors have superior estimates for means of security returns, they should prefer using them over the sample means calculated from historical data. Best and Grauer \cite{Best1,Best2} also arrived at similar conclusions by studying the sensitivity of weights of optimal portfolios with respect to changes in estimated means of returns on individual securities. Further, on imposition of no short selling constraint on the securities, they observe that a small change in estimated mean return of an individual security can assign zero weights to almost half the securities comprising a portfolio.\\

The discussed research works arrive at a common conclusion that the optimal portfolios are extremely sensitive towards the estimated values of input parameters, particularly expected returns on individual securities. In order to address this issue, there has been significant progress in recent years in the area of robust portfolio optimization. Several methods have been proposed in this area. \textbf{We are particularly interested in the approaches falling in the category related to enhancing robustness by optimizing the portfolio performance in worst-case scenarios.} \\

Significant efforts have been made towards formulating these kinds of approaches from Markowitz based mean-variance analysis. However, no rigorous framework has been observed that could establish these approaches at par with Markowitz optimization in terms of portfolio performance. Ceria and Stubbs \cite{Ceria} introduce a methodology for robust portfolio optimization, taking into account the estimation errors in input parameters while formulating the optimization problem. The approach involves minimizing the worst case return for a given confidence region. They observe that the constructed robust portfolios perform superior in comparison to those constructed using mean-variance analysis in most of the cases but not in each month with certainty. Utilizing the above framework, Scherer\cite{Scherer} shows that robust methods don't lead to significant change in the efficient set. Constructing an example, he shows that robust portfolio underperforms out of the sample in comparison to Markowitz portfolio, especially in the case of low risk aversion and high uncertainty aversion. He also argues that performance of robust portfolio is dependent upon the consistency between uncertainty aversion and risk aversion which is quite complicated. Santos\cite{santos} performs similar experiments and concludes stating better performance of robust optimization in comparison to the portfolios constructed using mean-variance analysis in the case of simulated data unlike the real market data.

\section{Robust Optimization}
All the real world optimizing problems inevitably have uncertain parameters embedded in them. In order to tackle such problems, a framework called \say{Stochastic Programming} \cite{stochastic_prog} is used, which can model such problems having uncertain parameters. These models take the probability distributions of the underlying data into consideration. To improve the stability of the solutions, robust methods such as re-sampling techniques, robust estimators and Bayesian approaches were developed. One of the approaches is \textbf{robust optimization}, which is used when the parameters are known to lie in a certain range. In this section, we explain some robust models with worst-case optimization approaches for a given objective function within a predefined \say{uncertainty} set. \\

The concept of uncertainty sets was introduced by Soyster \cite{soyster}, where he uses a different  definition for defining a feasible region of a convex programming problem. The convex inequalities are replaced by convex sets with a condition that the finite sum of convex sets again should be within another convex set. In another way, he defines a new linear programming problem(LPP) with uncertain truth value, but it is bound to lie within a defined convex set. Later, El Ghaoui and Lebret \cite{elg_lsq} extend these uncertainty sets to define a robust formulation while tackling the least-squares problem having uncertain parameters, but they are bounded matrices. In their work, they describe the problem of finding a worst-case residual and refer the solution as a robust least-squares solution. Furthermore, they show that it can be computed via semi-definite or second order cone programming. El Ghaoui, Oustry, and Lebret \cite{elg_semidefinite} further study how to integrate bounded uncertain parameters in semidefinite programming. They introduce robust-formulations for semidefinite programming and provide sufficient conditions to guarantee the existence of such robust solutions. Ben-Tal and Nemirovski \cite{bental_rc} mainly focus on the uncertainty related with \textit{hard} constraints and which are \textit{ought} to be satisfied, irrespective of the representation of the data. They suggest a methodology where they replace an actual uncertain linear programming problem by its robust counterpart. They show that the robust counterpart  of a linear programming problem with the ellipsoidal uncertainty set is computationally attractive, as it reduces to a polynomial time solvable conic quadratic program. Additionally, they use interior points methods \cite{bental_interior} to compute the solutions efficiently. Along the same lines, Goldfarb and Iyengar \cite{Goldfarb}, focus on the robust convex quadratically constrained programs which are a subclass of the robust convex programs of Ben-Tal and Nemirovski \cite{bental_rc}. They mainly work on finding uncertainty sets which structures this subclass of programs as second-order cone programs. \\

In its early phases, the major directions of research were to introduce robust formulations and to build uncertainty sets for robust counterparts of the LPP as they are computationally attractive. Once the basic framework of robust optimization was established, it is now applied across various domains such as learning, statistics, finance and numerous areas of engineering.

\section{Uncertainty sets}

The determination of the structure of the uncertainty sets, so as to obtain computationally tractable solutions has been a key step in robust optimization. In the real world, even in the distributions of asset returns, there is an uncertainty. In order to address this issue, a most frequently used technique is to find an estimate of the uncertain parameter and to define a geometrical bound around it. Empirically, historical returns are used to compute the estimates of these uncertain parameters. For a given optimization problem, determining the geometry of the uncertainty set is a difficult task. Accordingly, in this section, we discuss a couple of popular types of uncertainty sets which are used in portfolio optimization.

In literature, there are many extensions of uncertainty sets varying from simple polytopes to statistically derived conic-representable sets. A \textit{polytopic} \cite{fabozzi} uncertainty set which resembles a \say{box}, it is defined as
\begin{equation}
\label{eqn:box}
U_{\mathbf{\delta}}(\hat{\mathbf{a}}) = \left\{ \mathbf{a} : | a_i - \hat{a_i}| \leq \delta_i, i = 1,2,3,...,N \right\},
\end{equation}

where $\mathbf{a} = (a_i, a_2, ..., a_N)$ is a vector of values of uncertain parameters of dimension $N$ and $\mathbf{\hat{a}} = (\hat{a_1}, \hat{a_2}, ... , \hat{a_N})$ is generally the estimate for $\mathbf{a}$'s.

Another type of an uncertainty set which takes the second moment of the distribution into the account. Such type of sets are called \textit{ellipsoidal} uncertainty sets. One of the most popular way of defining them is
\begin{equation}
\label{eqn:ellipse}
U(\hat{\mathbf{a}}) = \left\{ \mathbf{a} : \mathbf{a} = \hat{\mathbf{a}} + \mathbf{P}^{1/2}\mathbf{u}, ||\mathbf{u}||\leq 1 \right\},
\end{equation}
where the choice of $\mathbf{P}$ is driven by the optimization problem. The main motivation to use these kind of sets is that they come up naturally when one tries to estimate uncertain parameters using techniques like regression, and these sets also take probabilistic properties into account. We further discuss how to model the uncertainties for some of the financial indicators.

\subsection{Uncertainty in expected returns}
Recently, many attempts were made to model the uncertainty in the expected returns because of the several reasons. When compared with variances and covariances, it is known that the effect on the performance of portfolio by the estimated error is more in case of expected returns. In case, even if the future returns of the assets are not equal to the estimated values, one can foresee that they will be with in certain range of the estimated return. Accordingly, one can define uncertainty sets in such a way so that expected values lie inside the geometric bound around the estimated value, say $\boldsymbol{\hat{\mu}}$.

In a simple scenario, one can define possible intervals for the expected returns of each individual asset by using box uncertainty set. Mathematically, it can be expressed as 
\begin{equation}
U_{\boldsymbol{\delta}}(\boldsymbol{\hat{\mu}}) = \left\{ \boldsymbol{\mu}: | \mu_i - \hat{\mu_i}| \leq \delta_i, i = 1,2,3,...,N \right\},    
\end{equation}
where $\delta_i$ represents the value which determines the confidence interval region for individual assets. Clearly from the above expression, for the asset $i$, the estimated error has an upper bound limit of $\delta_i$. Now, when we incorporate this uncertainty in the robust formulation with the only constraint being the sum of the weights equalling unity, the following max-min problem 
\begin{equation}
\label{eq:rf}
\max_{\mathbf{x}} \left\{ \min_{\boldsymbol{\mu} \in U_{\boldsymbol{\delta}}(\boldsymbol{\hat{\mu}})} \boldsymbol{\mu}^{\top} \mathbf{x} - \lambda \mathbf{x^{\top}}\Sigma \, \mathbf{x} \right\} \text{such that } \mathbf{x^{\top}}\mathbf{1}  = 1, 
\end{equation}
transforms to a maximization problem
\begin{equation}
\label{eqn:trans_eqn_box}
\max_\mathbf{x} \quad \boldsymbol{\hat{\mu}} \, \mathbf{x}-  \lambda \mathbf{x^{\top}}\Sigma \, \mathbf{x} - \boldsymbol{\delta}^{\top}|\mathbf{x}| \quad \text{such that } \mathbf{x^{\top}}\mathbf{1}  = 1,  
\end{equation}
where $\mathbf{1}$ represents the unity vector.
The most popular choice is to use ellipsoidal uncertainty set, as it takes the second moments into account. 
\begin{equation}
U_{\delta}(\boldsymbol{\hat{\mu}}) = \left\{ \boldsymbol{\mu} : (\boldsymbol{\mu} - \boldsymbol{\hat{\mu}})^{\top} \, \Sigma^{-1}_{\boldsymbol{\mu}} \, (\boldsymbol{\mu} - \boldsymbol{\hat{\mu}}) \leq \delta^2 \right\},
\end{equation}
where $\Sigma_\mu$ is a variance covariance matrix of the estimated error of expected returns of the assets.
Again, solving (\ref{eq:rf}) with ellipsoid uncertainty set yields
\begin{equation}
\max_{\mathbf{x}} \left\{ \boldsymbol{\hat{\mu}}^{\top} \mathbf{x} - \lambda \mathbf{x}^{\top} \, \Sigma_{\boldsymbol{\mu}} \, \mathbf{x} - \delta \sqrt{\mathbf{x}^{\top} \, \Sigma_{\boldsymbol{\mu}} \, \mathbf{x}} \right\} \text{such that } \mathbf{x^{\top}} \mathbf{1}  = 1.
\end{equation}
While dealing with the box uncertainty, it is assumed that the returns follow normal distribution as it eases the task of computing the desired confidence intervals for each individual assets. For the same reason, if the uncertainty set follows ellipsoid model, the underlying distribution is assumed to be tracing a $\chi^2$ distribution with number of assets being the degrees of freedom (df).

\subsection{Uncertainty in Covariance Matrix}
As mentioned earlier, portfolio performance is more sensitive towards estimation error in mean returns of assets in comparison to variances and covariances of asset returns. This is one of the major reasons behind research works laying less emphasis upon the uncertainty set for covariance matrix of asset returns. Box uncertainty set for covariance matrix of returns is defined on similar lines as that for expected returns. Lower bound $\underline{\Sigma}_{ij}$ and upper bound $\overline{\Sigma}_{ij}$ can be specified for each entry $\Sigma_{ij}$ of the covariance matrix. Using this methodology, the constructed box uncertainty set for covariance matrix is of the following form:
\begin{equation}
\begin{split}
& U_{\Sigma}= \{\Sigma: \underline{\Sigma} \leq \Sigma \leq \overline{\Sigma}, \ \Sigma \succeq 0 \}. \\
\end{split}
\end{equation}
In the above equation, the condition $\Sigma \succeq 0$ implies that $\Sigma$ is a symmetric positive semidefinite matrix. This condition is necessary in most of the robust optimization approaches, particularly those involving Markowitz model as the basic theoretical framework. \\\\
Tütüncü and Koenig \cite{tutuncu} discuss a method for solving the robust formulation of Markowitz optimization problem having non-negativity constraints upon the weights of assets. They define the uncertainty set for covariance matrix as per above equation and uncertainty set for expected returns as $U_{\boldsymbol{\mu}}= \{\boldsymbol{\mu}: \underline{\boldsymbol{\mu}} \leq \boldsymbol{\mu} \leq \overline{\boldsymbol{\mu}} \}$, where $\underline{\boldsymbol{\mu}}$ and $\overline{\boldsymbol{\mu}}$ represent lower and upper bounds on mean return vector $\boldsymbol{\mu}$ respectively . Accordingly, the robust optimization problem 
\begin{equation}
\begin{split}
& \max_{\mathbf{x}} \left\{ \min_{ (\boldsymbol{\mu},\Sigma) \in (U_{\boldsymbol{\mu}},U_{\Sigma})} \boldsymbol{\mu}^{\top} \mathbf{x} - \lambda \mathbf{x^{\top}}\Sigma \, \mathbf{x} \right\} \text{such that } \mathbf{x^{\top}} \mathbf{1}  = 1 \text{ and } \mathbf{x} \geq 0,  \\
\end{split}
\end{equation}
can be formulated as following:
\begin{equation}
\begin{split}
& \max_{\mathbf{x}} \left\{ \underline{\boldsymbol{\mu}}^{\top} \mathbf{x} - \lambda \mathbf{x^{\top}} \, \overline{\Sigma} \, \mathbf{x} \right\} \text{such that } \mathbf{x^{\top}} \mathbf{1}  = 1 \text{ and } \mathbf{x} \geq 0.  \\
\end{split}
\end{equation}
\subsection{Joint Uncertainty Set}
The discussed robust optimization approach formulated by Tütüncü and Koenig involves the use of \say{separable} uncertainty sets, which implies, the uncertainty sets for expected returns and covariance matrix are defined independent of each other. Lu \cite{lu} argues that there are certain drawbacks associated with separable uncertainty sets. Such kind of uncertainty sets don't take the knowledge of actual confidence level into consideration. Secondly, separable uncertainty sets don't incorporate the joint behavior of mean returns and covariance matrix. As a result, these uncertainty sets are fully or partially box-type. This is one of the major reasons behind robust portfolios being conservative or highly non-diversified as observed in numerous computations. In order to address these drawbacks, Lu proposes a joint uncertainty set. This uncertainty set is constructed as per desired confidence level using a statistical procedure that takes the factor model \cite{goldfarb2} for asset returns into consideration.\\

Delage and Ye \cite{delage} define a joint uncertainty set that takes into account the uncertainty in distribution of asset returns as well as moments (mean returns and covariance matrix of returns). The proposed uncertainty set having confidence parameters, $\gamma_{1} \geq 0$ and $\gamma_{2} \geq 1$, is given by:
\begin{equation}
\begin{split}
    & (\boldsymbol{\mu} - \boldsymbol{\hat{\mu}})^{\top} \ \hat{\Sigma}^{-1} \ (\boldsymbol{\mu} - \boldsymbol{\hat{\mu}}) \leq \gamma_{1}, \\
    & E[(\mathbf{r} - \boldsymbol{\hat{\mu}})(\mathbf{r} - \boldsymbol{\hat{\mu}})^{\top}] \leq \gamma_{2}\hat{\Sigma}. \\
\end{split}
\end{equation}
In the above equation, $\boldsymbol{\hat{\mu}}$ and $\hat{\Sigma}$ represent the estimates of mean return vector and covariance matrix of asset returns, and $\mathbf{r}$ is the random return vector. Using this uncertainty set, they formulate the portfolio optimization problem as a Distributionally Robust Stochastic Program (DRSR). Accordingly, they demonstrate that the problem is computationally tractable by solving it as a semidefinite program. 
\renewcommand\refname{Bibliography}

\bibliographystyle{unsrt} 
\bibliography{cite}

\end{document}















